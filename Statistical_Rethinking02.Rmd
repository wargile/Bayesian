---
title: "Statistical_Rethinking02"
author: "Robert A. Stevens"
date: "May 4, 2016"
output: html_document
---

```{r, comment=NA}
library(rethinking)
library(StanHeaders)
```

*Statistical Rethinking: A Bayesian Course with Examples in R and Stan*

by Richard McElreath

# 2.0 Small Worlds and Large Worlds  

Figure 2.1. Illustration of Martin Behaim's 1492 globe, showing the small world that Colombo anticipated. Europe lies on the right-hand side. Asia lies on the left. The big island labeled "Cipangu" is Japan.

Rethinking:  Fast and frugal in the large world

## 2.1 The garden of forking data  

### 2.1.1 Counting possibilities

Figure 2.2. The 64 possible paths generate by assuming the bag contains one blue and three white marbles.

Figure 2.3. After eliminating paths inconsistent with the observed sequence, only 3 of the 64 paths remain.

Figure 2.4. The garden of forking data, showing for each possible composition of the bab the forking paths that are logically compatible with the data.

Rethinking:  Justification

### 2.1.2 Using prior information

Rethinking:  Original ignorance

### 2.1.3 From counts to probability

```{r, comment=NA}
ways <- c(0, 3, 8, 9, 0)
ways/sum(ways)
```

Rethinking: Randomization

## 2.2 Building a model  

### 2.1.1 A data story

Rethinking: The value of storytelling

### 2.2.2 Bayesian updating

Figure 2.5. How a Bayesian model learns. Each toss of the globe produces an observations of water (W) or land (L). The model's estimate of the proportion of water on the globe is a plausibility for every possible value. The lines and curves in this figure are these collections of plausibilities. In each plot, previous plausibilities (dashed curve) are updated in light of the latest observation to produce a new set of plausibilities (solid curves).

Rethinking: Sample size and reliable inference

Rethinking: Deflationary statistics

### 2.2.3 Evaluate

## 2.3 Components of the model

### 2.3.1 Likelihood

R code 2.2
```{r, comment=NA}
dbinom(6, size = 9, prob = 0.5)
```

Overthinking: Names and probability distributions

Rethinking: A central role for likelihood

### 2.3.2 Parameters

Rethinking: Data or parameter

### 2.3.3 Prior

Overthinking: Prior as probability distribution

Rethinking: Prior, prior pants on fire

### 2.3.4 Posterior

Figure 2.6. the posterior distribution, as a produce of the prior distribution and likelihood. Top row: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle row: A step prior, assigning zero probability to all values less than 0.5, resulting in a truncated posterior. Bottom row: A peaked prior that shifts and skew the posterior, relative to the likelihood.

Rethinking: Bayesian data analysis isn't about Bayes' theorem

## 2.4 Making the model go  

Rethinking: How you fit the model is part of the model

### 2.4.1 Grid approximation

R code 2.3
```{r, comment=NA}
# define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# define prior
prior <- rep(1, 20)

# compute likelihood at each value in grid
likelihood <- dbinom(6, size = 9, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood*prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior/sum(unstd.posterior)
```

R code 2.4
```{r, comment=NA}
plot(p_grid, posterior, type = "b",
     xlab = "probability of water", ylab = "posterior probability")
mtext("20 points")
```

Figure 2.7. Computing posterior distribution by grid approximation. In each plot, the posterior distribution for the globe toss data and model is approximated with a finite number of evenly spaced points. With only 5 points (left), the approximation is terrible. But with 20 points (right), the approximation is already quite good. Compare to the analytically solved, exact posterior distribution in Figure 2.5 (page 30).

R code 2.5
```{r, comment=NA}
prior <- ifelse(p_grid < 0.5, 0, 1)
prior <- exp(-5*abs(p_grid - 0.5))
```

Overthinking: Vectorization

### 2.4.2 Quadratic approximation

R code 2.6
```{r, comment=NA}
globe.qa <- map(
  alist(
    w ~ dbinom(9, p) ,  # binomial likelihood
    p ~ dunif(0, 1)     # uniform prior
  ) ,
  data = list(w = 6) 
)

# display summary of quadratic approximation
precis(globe.qa)
```

R code 2.7
```{r, comment=NA}
# analytical calculation
w <- 6
n <- 9
curve(dbeta(x, w + 1, n - w + 1), from = 0, to = 1)
# quadratic approximation
curve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)
```

Figure 2.8 Accuracy of the quadratic approximation. In each plot, the exact posterior distribution is plotted in blue, and the quadratic approximation is plotted as the black curve. Left: The globe tossing data with n = 9 tosses and w = 6 waters. Middle: Double the amount of data, with the same fraction of water, n = 18 and w = 12. Right: Four times as much data, n = 36 and w = 24.

Rethinking: Maximum likelihood estimation

Overthinking: The Hessians are coming

### 2.4.3 Markov chain Monte Carlo

## 2.5 Summary   

## 2.6 Practice   

2E1

2E2

2E3

2E4

2M1

2M2

2M3

2M4

2M5

2M6

2M7

2H1

2H2

2H3

2H4
