---
title: "Statistical_Rethinking12"
author: "Robert A. Stevens"
date: "May 4, 2016"
output: html_document
---

```{r, comment=NA}
library(rethinking)
library(StanHeaders)
```

*Statistical Rethinking: A Bayesian Course with Examples in R and Stan*

by Richard McElreath

# 12.0 Multilevel Models  

Rethinking:  A model by any other name

## 12.1 Example: Multilevel tadpoles

```{r, comment=NA}
data(reedfrogs)
d <- reedfrogs
str(d)
```

```{r, comment=NA}
#data(reedfrogs)
#d <- reedfrogs

# make the tank cluster variable
d$tank <- 1:nrow(d)

# fit
m12.1 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank] ~ dnorm(0, 5)
  ),
  data = d)
precis(m12.1, depth = 2)
```

Rethinking: Why Gaussian tanks?

```{r, comment=NA}
m12.2 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank] ~ dnorm( a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1)
  ), data = d, iter = 4000, chains = 4)
precis(m12.2, depth = 2)
```

```{r, comment=NA}
compare( m12.1 , m12.2 )
```

Overthinking: MAP fails, MCMC wins

```{r, comment=NA}
# extract Stan samples
post <- extract.samples(m12.2)

# compute median intercept for each tank
# also transform to probability with logistic
d$propsurv.est <- logistic(apply(post$a_tank, 2, median))

# display raw proportions surviving in each tank
plot(d$propsurv, ylim = c(0, 1), pch = 16, xaxt = "n",
    xlab = "tank", ylab = "proportion survival", col = rangi2)
axis(1, at = c(1, 16, 32, 48), labels = c(1, 16, 32, 48))

# overlay posterior medians
points(d$propsurv.est)

# mark posterior median probability across tanks
abline(h = logistic(median(post$a)), lty = 2)

# draw vertical dividers between tank densities
abline(v = 16.5, lwd = 0.5)
abline(v = 32.5, lwd = 0.5)
text(     8, 0, "small tanks")
text(16 + 8, 0, "medium tanks")
text(32 + 8, 0, "large tanks")
```

Figure 12.1. Empirical proportions of survivors in each tadpole tank, shown by the filled blue points, plotted with the 48 per-tank estimates from the multilevel model, shown by the black circles. The dashed line locates the overall averaged proportion of survivors across the tanks. The vertical lines divide tanks with different initial densities of tadpoles: small tanks (10 tadpoles), medium tanks (25), and large tanks (35). In every tank, the posterior median from the multilevel models is closer to the dashed line than the empirical proportion is. This reflects the pooling of information across tanks, to help with inference about each tank.

```{r, comment=NA}
# show first 100 populations in the posterior
plot(NULL, xlim = c(-3, 4), ylim = c(0, 0.35),
     xlab = "log-odds survive", ylab = "Density")
for(i in 1:100)
  curve(dnorm(x, post$a[i], post$sigma[i]), add = TRUE,
        col = col.alpha("black", 0.2))

# sample 8000 imaginary tanks from the posterior distribution
sim_tanks <- rnorm(8000, post$a, post$sigma)

# transform to probability and visualize
dens(logistic(sim_tanks), xlab = "probability survive")
```

Figure 12.2. The inferred population of survival across tanks. Left: 100 Gaussian distributions of the log-odds of survival, sampled from the posterior of m12.2. Right: Survival probabilities for 8000 new simulated tanks, averaging over the posterior distribution of the left.

Rethinking: Varying intercepts as over-dispersion.

Overthinking: Priors for variance components.

## 12.2 Varying effects and the underfitting/overfitting trade-off  

### 12.2.1 The model

### 12.2.2 Assign values to the parameters

```{r, comment=NA}
a <- 1.4
sigma <- 1.5
nponds <- 60
ni <- as.integer(rep(c(5, 10, 25, 35), each = 15))
```

```{r, comment=NA}
a_pond <- rnorm(nponds, mean = a, sd = sigma)
```

```{r, comment=NA}
dsim <- data.frame(pond = 1:nponds, ni = ni, true_a = a_pond)
```

Overthinking: Data types and Stan models

```{r, comment=NA}
class(1:3)
class(c(1, 2, 3))
```

### 12.2.3 Simulate survivors

```{r, comment=NA}
dsim$si <- rbinom(nponds, prob = logistic(dsim$true_a), size = dsim$ni)
```

### 12.2.4 Compute the no-pooling estimates

```{r, comment=NA}
dsim$p_nopool <- with(dsim, si/ni)
```

### 12.2.5 Compute the partial-pooling estimates

```{r, comment=NA}
m12.3 <- map2stan(
  alist(
    si ~ dbinom(ni , p),
    logit(p) <- a_pond[pond],
    a_pond[pond] ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1)
  ),
  data = dsim, iter = 1e4, warmup = 1000)
```

```{r, comment=NA}
precis(m12.3, depth = 2)
```

```{r, comment=NA}
estimated.a_pond <- as.numeric(coef(m12.3)[1:60])
dsim$p_partpool <- logistic(estimated.a_pond)
```

```{r, comment=NA}
dsim$p_true <- logistic(dsim$true_a)
```

```{r, comment=NA}
nopool_error <- with(dsim, abs(p_nopool - p_true))
partpool_error <- with(dsim, abs(p_partpool - p_true))
```

```{r, comment=NA}
plot(1:60, nopool_error, xlab = "pond", ylab = "absolute error",
     col = rangi2, pch = 16)
points(1:60, partpool_error)

# draw vertical dividers between tank densities
abline(v = 15, lwd = 0.5)
abline(v = 30, lwd = 0.5)
abline(v = 45, lwd = 0.5)
text(     7.5, 0.575, "tiny (5)")
text(15 + 7.5, 0.575, "small (10)")
text(30 + 7.5, 0.575, "medium (25)")
text(45 + 7.5, 0.575, "large (35)")
```

Figure 12.3. Error of no-pooling and partial pooling estimates, for the simulated tadpole ponds. The horizontal axis displays pond number. The vertical axis measures the absolute error in the predicted proportion of survivors, compared to the true value used in the simulation. The higher the point, the worse the estimate. No-pooling shown in blue. Partial pooling shown in black. The blue and dashed black lines show the average error for each kind of estimate, across each initial density of tadpoles (pond size). Smaller ponds produce more error, but the partial pooling estimates are better on average, especially in smaller ponds.

Overthinking: Repeating the pond simulation.

```{r, comment=NA}
a <- 1.4
sigma <- 1.5
nponds <- 60
ni <- as.integer(rep(c(5, 10, 25, 35), each = 15))
a_pond <- rnorm(nponds, mean = a, sd = sigma)
dsim <- data.frame(pond = 1:nponds, ni = ni, true_a = a_pond)
dsim$si <- rbinom(nponds, prob = logistic(dsim$true_a), size = dsim$ni)
dsim$p_nopool <- with(dsim, si/ni)
newdat <- list(si = dsim$si, ni = dsim$ni, pond = 1:nponds)
m12.3new <- map2stan(m12.3, data = newdat, iter = 1e4, warmup = 1000)
precis(m12.3new, depth = 2)
```

## 12.3 More than one type of cluster  

Rethinking: Cross-classification and hierarchy

### 12.3.1 Multilevel chimpanzees

```{r, comment=NA}
y1 <- rnorm(1e4, 10, 1)
y2 <- 10 + rnorm(1e4, 0, 1)
```

```{r, comment=NA}
data(chimpanzees)
d <- chimpanzees
d$recipient <- NULL # get rid of NAs

m12.4 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a + a_actor[actor] + (bp + bpC*condition)*prosoc_left,
    a_actor[actor] ~ dnorm(0, sigma_actor),
    a ~ dnorm(0, 10),
    bp ~ dnorm(0, 10),
    bpC ~ dnorm(0, 10),
    sigma_actor ~ dcauchy(0, 1)
  ),
  data = d, warmup = 1000, iter = 5000, chains = 4, cores = 3)
precis(m12.4, depth = 2)
```

```{r, comment=NA}
post <- extract.samples(m12.4)
total_a_actor <- sapply(1:7, function(actor) post$a + post$a_actor[ , actor])
round(apply(total_a_actor, 2, mean), 2)
```

### 12.3.2 Two types of cluster

```{r, comment=NA}
# prep data
d$block_id <- d$block # name 'block' is reserved by Stan

m12.5 <- map2stan(
  alist(
    pulled_left ~ dbinom( 1 , p ),
    logit(p) <- a + a_actor[actor] + a_block[block_id] +
                (bp + bpc*condition)*prosoc_left,
    a_actor[actor] ~ dnorm(0, sigma_actor),
    a_block[block_id] ~ dnorm(0, sigma_block),
    c(a,bp,bpc) ~ dnorm(0, 10),
    sigma_actor ~ dcauchy(0, 1),
    sigma_block ~ dcauchy(0, 1)
  ),
  data = d, warmup = 1000, iter = 6000, chains = 4, cores = 3)
```

```{r, comment=NA}
precis(m12.5, depth = 2) # depth = 2 displays varying effects
plot(precis(m12.5, depth = 2)) # also plot
```

Figure 12.4. Left: Posterior means and 89% highest density intervals for m12.5. The greater variation across actors than blocks can be seen immediately in the a\_actor and a\_block distributions. Right: Posterior distributions of the standard deviations of varying intercepts by actor (blue) and experimental block (black).

```{r, comment=NA}
post <- extract.samples(m12.5)
dens(post$sigma_block, xlab = "sigma", xlim = c(0, 4))
dens(post$sigma_actor, col = rangi2, lwd = 2, add = TRUE)
text(2.00, 0.85, "actor", col = rangi2)
text(0.75, 2.00, "block")
```

```{r, comment=NA}
compare(m12.4,m12.5)
```

### 12.3.3 Even more clusters

## 12.4 Multilevel posterior predictions  

### 12.4.1 Posterior prediction for same clusters

```{r, comment=NA}
chimp <- 2
d.pred <- list(
    prosoc_left = c(0, 1, 0, 1), # right/left/right/left
    condition = c(0, 0, 1, 1),   # control/control/partner/partner
    actor = rep(chimp, 4)
)
link.m12.4 <- link(m12.4, data = d.pred)
pred.p <- apply(link.m12.4, 2, mean)
pred.p.PI <- apply(link.m12.4, 2, PI)
```

```{r, comment=NA}
post <- extract.samples(m12.4)
str(post)
```

```{r, comment=NA}
dens(post$a_actor[ , 5])
```

```{r, comment=NA}
p.link <- function(prosoc_left, condition, actor) {
  logodds <- with(post,
    a + a_actor[ , actor] + (bp + bpC*condition)*prosoc_left
  )
  return(logistic(logodds))
}
```

```{r, comment=NA}
prosoc_left <- c(0, 1, 0, 1)
condition <- c(0, 0, 1, 1)
pred.raw <- sapply(1:4, function(i) p.link(prosoc_left[i], condition[i], 2))
pred.p <- apply(pred.raw, 2, mean)
pred.p.PI <- apply(pred.raw, 2, PI)
```

### 12.4.2 Posterior prediction for new clusters

```{r, comment=NA}
d.pred <- list(
    prosoc_left = c(0, 1, 0, 1), # right/left/right/left
    condition = c(0, 0, 1, 1),   # control/control/partner/partner
    actor = rep(2,4))            # placeholder
```

```{r, comment=NA}
# replace varying intercept samples with zeros
# 1000 samples by 7 actors
a_actor_zeros <- matrix(0, 1000, 7)
```

```{r, comment=NA}
# fire up link
# note use of replace list
link.m12.4 <- link(m12.4, n = 1000, data = d.pred,
  replace = list(a_actor = a_actor_zeros))

# summarize and plot
pred.p.mean <- apply(link.m12.4, 2, mean)
pred.p.PI <- apply(link.m12.4, 2, PI, prob = 0.8)
plot(0, 0, type = "n", xlab = "prosoc_left/condition",
     ylab = "proportion pulled left", ylim = c(0, 1), xaxt = "n",
     xlim = c(1, 4))
axis(1, at = 1:4, labels = c("0/0", "1/0", "0/1", "1/1"))
lines(1:4, pred.p.mean)
shade(pred.p.PI, 1:4)
```

Figure 12.5. Posterior predictive distributions for the chimpanzees varying intercept model, m12.4. The solid lines are posterior means and the shaded regions are 80% percentile intervals. Left: Setting the varying intercept a\_actor to zero produces predictions for an average actor. These predictions ignore uncertainty arising from variation among actors. Middle: Simulating varying intercepts using the posterior standard deviation among actors, sigma\_actor, produces predictions that account for variation among actors.

```{r, comment=NA}
# replace varying intercept samples with simulations
post <- extract.samples(m12.4)
a_actor_sims <- rnorm(7000, 0, post$sigma_actor)
a_actor_sims <- matrix(a_actor_sims, 1000, 7)
```

```{r, comment=NA}
link.m12.4 <- link(m12.4, n = 1000, data = d.pred,
  replace = list(a_actor = a_actor_sims))
```

```{r, comment=NA}
post <- extract.samples(m12.4)
sim.actor <- function(i) {
  sim_a_actor <- rnorm(1, 0, post$sigma_actor[i])
  P <- c(0, 1, 0, 1)
  C <- c(0, 0, 1, 1)
  p <- logistic(post$a[i] + sim_a_actor + (post$bp[i] + post$bpC[i]*C)*P)
  return(p)
}
```

```{r, comment=NA}
# empty plot
plot(0, 0, type = "n", xlab = "prosoc_left/condition",
     ylab = "proportion pulled left", ylim = c(0, 1), xaxt = "n", xlim = c(1,4))
axis(1, at = 1:4, labels = c("0/0", "1/0", "0/1", "1/1"))

# plot 50 simulated actors
for(i in 1:50) lines(1:4, sim.actor(i), col = col.alpha("black", 0.5))
```

Figure 12.5. Posterior predictive distributions for the chimpanzees varying intercept model, m12.4. The solid lines are posterior means and the shaded regions are 80% percentile intervals. Right: 50 simulated actors with unique intercepts sampled from the posterior. Each simulation maintains the same parameter values across all four treatments.

### 12.4.3 Focus and multilevel prediction

```{r, comment=NA}
# prep data
data(Kline)
d <- Kline
d$logpop <- log(d$population)
d$society <- 1:10

# fit model
m12.6 <- map2stan(
  alist(
    total_tools ~ dpois(mu),
    log(mu) <- a + a_society[society] + bp*logpop,
    a ~ dnorm(0, 10),
    bp ~ dnorm(0, 1),
    a_society[society] ~ dnorm(0,sigma_society),
    sigma_society ~ dcauchy(0, 1)
  ),
  data = d, iter = 4000, chains = 3)
precis(m12.6, depth = 2) 
```

```{r, comment=NA}
post <- extract.samples(m12.6)
d.pred <- list(
  logpop = seq(from = 6, to = 14, length.out = 30), society = rep(1, 30)
)
a_society_sims <- rnorm(20000, 0, post$sigma_society)
a_society_sims <- matrix(a_society_sims, 2000, 10)
link.m12.6 <- link(m12.6, n = 2000, data = d.pred,
  replace = list(a_society = a_society_sims))
```

```{r, comment=NA}
# plot raw data
plot(d$logpop, d$total_tools, col = rangi2, pch = 16,
     xlab = "log population", ylab = "total tools")

# plot posterior median
mu.median <- apply(link.m12.6, 2, median)
lines(d.pred$logpop, mu.median)

# plot 97%, 89%, and 67% intervals (all prime numbers)
mu.PI <- apply(link.m12.6, 2, PI, prob = 0.97)
shade(mu.PI, d.pred$logpop)
mu.PI <- apply(link.m12.6, 2, PI, prob = 0.89)
shade(mu.PI, d.pred$logpop)
mu.PI <- apply(link.m12.6, 2, PI, prob = 0.67)
shade(mu.PI, d.pred$logpop)
```

Figure 12.6. Posterior predictions for the over-dispersed Poisson island model, m12.6. The shaded regions are, inside to out: 67%, 89%, and 97% intervals of the expected mean. Marginalizing over the varying intercepts results in a much wider prediction region than we'd expect under a pure Poisson process.

## 12.5 Summary  

## 12.6 Practice 

12E1.

12E2.

12E3.

12E4.

12E5.

12M1.

12M2.

12M3.

12M4.

12H1.

```{r, comment=NA}
data(bangladesh)
d <- bangladesh
sort(unique(d$district))
```

```{r, comment=NA}
d$district_id <- as.integer(as.factor(d$district))
sort(unique(d$district_id))
```

12H2.

12H3.
